{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, action_space, state_space):\n",
    "        self.action_space = action_space  # Lista de ações possíveis\n",
    "        self.state_space = state_space  # Lista de estados possíveis\n",
    "        self.q_table = np.zeros((len(state_space), len(action_space)))  # Q-Learning (Podemos mudar pra D-Learning visto em aula 16/09)\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1  # Exploração\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.action_space)  # Explorar\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Exploitar (melhor ação)\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.max(self.q_table[next_state])\n",
    "        self.q_table[state, action] = self.q_table[state, action] + self.learning_rate * (reward + self.discount_factor * best_next_action - self.q_table[state, action])\n",
    "\n",
    "# Agente codificador (Exemplos)\n",
    "action_space = [\"limpar_dados\", \"analisar_dados\", \"visualizar_resultados\"]\n",
    "state_space = [\"dados_brutos\", \"dados_processados\", \"resultados_gerados\"]\n",
    "\n",
    "codificador = Agent(action_space, state_space)\n",
    "# Agente revisor (Exemplos)\n",
    "action_space_revisor = [\"executar_codigo\", \"propor_refatoracao\", \"analisar_estaticamente\", \"aprovar_codigo\"]\n",
    "state_space_revisor = [\"codigo_gerado\", \"codigo_com_erro\", \"codigo_pronto\"]\n",
    "\n",
    "revisor = Agent(action_space_revisor, state_space_revisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ambiente:\n",
    "    def __init__(self, codificador, revisor, task):\n",
    "        self.codificador = codificador\n",
    "        self.revisor = revisor\n",
    "        self.task = task  # Definir uma tarefa de análise de dados (por exemplo, limpeza de dados)\n",
    "    \n",
    "    def executar_ciclo(self):\n",
    "        state = self.task.initial_state()\n",
    "        \n",
    "        # Ciclo de codificação\n",
    "        action = self.codificador.choose_action(state)\n",
    "        next_state, reward_codificador = self.task.executar_acao_codificador(action)\n",
    "        self.codificador.update_q_table(state, action, reward_codificador, next_state)\n",
    "        \n",
    "        # Ciclo de revisão\n",
    "        action_revisor = self.revisor.choose_action(next_state)\n",
    "        final_state, reward_revisor = self.task.executar_acao_revisor(action_revisor)\n",
    "        self.revisor.update_q_table(next_state, action_revisor, reward_revisor, final_state)\n",
    "        \n",
    "        return final_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset  # Dataset que vamos usar\n",
    "        self.state = \"dados_brutos\" # Estado inicial\n",
    "    \n",
    "    def initial_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    # Exemplo de ciclo com penalidade e recompensa (Vamos deixar mais complexo inserindo penalidade por caminhos mais longos no futuro)\n",
    "    def executar_acao_codificador(self, action):\n",
    "        if action == \"limpar_dados\":\n",
    "            # Lógica de limpeza dos dados\n",
    "            self.state = \"dados_processados\"\n",
    "            reward = 10  # Exemplo de recompensa\n",
    "        elif action == \"analisar_dados\":\n",
    "            self.state = \"resultados_gerados\"\n",
    "            reward = 20\n",
    "        else:\n",
    "            reward = -10  # Penalidade por erro\n",
    "        return self.state, reward\n",
    "    \n",
    "    def executar_acao_revisor(self, action):\n",
    "        if action == \"executar_codigo\":\n",
    "            # Simular execução do código gerado\n",
    "            reward = 5\n",
    "        elif action == \"propor_refatoracao\":\n",
    "            reward = 15\n",
    "        else:\n",
    "            reward = -5  # Penalidade\n",
    "        return self.state, reward\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
